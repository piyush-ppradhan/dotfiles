-- require("llm").setup({
-- 	backend = "ollama",
-- 	model = "tinyllama:1.1b",
-- 	url = "http://localhost:11434/api/generate",
-- 	request_body = {
-- 		options = {
-- 			temperature = 0.2,
-- 			top_p = 0.95,
-- 		}
-- 	}
-- })
